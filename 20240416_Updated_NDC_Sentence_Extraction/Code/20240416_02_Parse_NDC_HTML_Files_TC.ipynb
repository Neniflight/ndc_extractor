{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23c298-d255-4579-b6bc-e405e54b5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Directory and file setup\n",
    "directory_path = '/Users/twc/Research/20221022_Updated_NDCs/Data/20240117_ClimateWatch_AllData/NDC_text_HTML/ndc-master'\n",
    "file_path = '/Users/twc/Research/20221022_Updated_NDCs/Data/20240118_countries.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# List to hold content\n",
    "html_contents = []\n",
    "\n",
    "# Load and parse HTML files\n",
    "for filename in data['Climate.Watch.HTML.File']:\n",
    "    full_path = os.path.join(directory_path, filename)\n",
    "    if os.path.exists(full_path):\n",
    "        with open(full_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "            html_contents.append(soup)\n",
    "    else:\n",
    "        html_contents.append(None)\n",
    "\n",
    "# Function to find the closest preceding h1, h2, h3, h4 for any tag and replace line breaks\n",
    "def find_preceding_tags(tag):\n",
    "    previous_elements = tag.find_all_previous()\n",
    "    found_tags = {}\n",
    "    for element in previous_elements:\n",
    "        if element.name in ['h1', 'h2', 'h3', 'h4'] and element.name not in found_tags:\n",
    "            text = element.get_text(strip=True).replace('\\n', ' ').replace('\\r', ' ')\n",
    "            found_tags[element.name] = text\n",
    "        if len(found_tags) == 4:\n",
    "            break\n",
    "    return found_tags  # Returns a dictionary of the found headings\n",
    "\n",
    "def extract_text(tag):\n",
    "    if tag.name == 'table':\n",
    "        return ' '.join([cell.get_text(strip=True).replace('\\n', ' ').replace('\\r', ' ') for cell in tag.find_all('td')])\n",
    "    elif tag.name in ['ol', 'ul']:\n",
    "        return ' '.join([item.get_text(strip=True).replace('\\n', ' ').replace('\\r', ' ') for item in tag.find_all('li')])\n",
    "    else:  # 'p' and other direct text containers\n",
    "        return tag.get_text(strip=True).replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "# Extracting data\n",
    "all_data = []\n",
    "for i, soup in enumerate(html_contents):\n",
    "    if soup is not None:\n",
    "        tags = soup.find_all(['p', 'ol', 'ul', 'table'])\n",
    "        for tag in tags:\n",
    "            tag_text = extract_text(tag)\n",
    "            headings = find_preceding_tags(tag)\n",
    "            all_data.append({\n",
    "                'iso': data.loc[i, 'ISO'],\n",
    "                'country': data.loc[i, 'Country'],\n",
    "                'ndc': data.loc[i, 'NDC'],\n",
    "                'date': data.loc[i, 'Date'],\n",
    "                'html': data.loc[i, 'Climate.Watch.HTML.File'],\n",
    "                'text_type': tag.name,\n",
    "                'text_content': tag_text,\n",
    "                'h1_text': headings.get('h1', None),\n",
    "                'h2_text': headings.get('h2', None),\n",
    "                'h3_text': headings.get('h3', None),\n",
    "                'h4_text': headings.get('h4', None),\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "df_elements = pd.DataFrame(all_data)\n",
    "print(df_elements)\n",
    "\n",
    "elements_file_path = '/Users/twc/Research/20221022_Updated_NDCs/Output/20240416_02_NDC_elements.csv'\n",
    "df_elements.to_csv(elements_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ccf8e-8a34-4326-8fd2-7d54c7e8e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Assuming df_elements is already loaded with the necessary data\n",
    "\n",
    "# Downloading the required resource for nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Expanding the DataFrame to include a row per sentence\n",
    "all_rows = []\n",
    "for _, row in df_elements.iterrows():\n",
    "    sentences = split_into_sentences(row['text_content'])\n",
    "    for sentence in sentences:\n",
    "        new_row = row.copy()\n",
    "        new_row['text_content'] = sentence\n",
    "        all_rows.append(new_row)\n",
    "\n",
    "# Create new DataFrame\n",
    "df_sentences = pd.DataFrame(all_rows)\n",
    "\n",
    "print(df_sentences)\n",
    "\n",
    "sentences_file_path = '/Users/twc/Research/20221022_Updated_NDCs/Output/20240416_02_NDC_sentences.csv'\n",
    "df_sentences.to_csv(sentences_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe54971-8716-488e-9684-539c92d44dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
